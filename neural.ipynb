{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup üèóÔ∏è\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "viscos = 1/5200\n",
    "\n",
    "# load RANS data created by rans.m (which can be downloaded)\n",
    "# load DNS data\n",
    "DNS_mean  = np.genfromtxt(\"LM_Channel_5200_mean_prof.dat\",comments=\"%\").transpose()\n",
    "y_DNS     = DNS_mean[0]\n",
    "yplus_DNS = DNS_mean[1]\n",
    "u_DNS     = DNS_mean[2]\n",
    "dudy_DNS  = np.gradient(u_DNS,y_DNS)\n",
    "\n",
    "DNS_stress = np.genfromtxt(\"LM_Channel_5200_vel_fluc_prof.dat\",comments=\"%\").transpose()\n",
    "\n",
    "uu_DNS = DNS_stress[2]\n",
    "vv_DNS = DNS_stress[3]\n",
    "ww_DNS = DNS_stress[4]\n",
    "uv_DNS = DNS_stress[5]\n",
    "uw_DNS = DNS_stress[6]\n",
    "vw_DNS = DNS_stress[7]\n",
    "k_DNS  = 0.5*(uu_DNS+vv_DNS+ww_DNS)\n",
    "\n",
    "DNS_RSTE = np.genfromtxt(\"LM_Channel_5200_RSTE_k_prof.dat\",comments=\"%\")\n",
    "\n",
    "eps_DNS = DNS_RSTE[:,7]/viscos # it is scaled with ustar**4/viscos\n",
    "\n",
    "# fix wall\n",
    "eps_DNS[0]=eps_DNS[1]\n",
    "\n",
    "# load data from k-omega RANS\n",
    "data  = np.loadtxt('y_u_k_om_uv_5200-RANS-code.txt').transpose()\n",
    "y     = data[0]\n",
    "u     = data[1]\n",
    "k     = data[2]\n",
    "om    = data[3]\n",
    "diss1 = 0.09*k*om\n",
    "ustar = (viscos*u[0]/y[0])**0.5\n",
    "yplus = y*ustar/viscos\n",
    "\n",
    "# dont train on, uu, vv, ww, uv, uw, vw \n",
    "# Maybe mixed terms are ok (just not uu,vv,ww)\n",
    "\n",
    "#-----------------Data_manipulation--------------------\n",
    "\n",
    "# Delete first value for all interesting data\n",
    "uv_DNS    = np.delete(uv_DNS, 0)\n",
    "vv_DNS    = np.delete(vv_DNS, 0)\n",
    "ww_DNS    = np.delete(ww_DNS, 0)\n",
    "uw_DNS    = np.delete(uw_DNS,0)\n",
    "vw_DNS    = np.delete(vw_DNS,0)\n",
    "k_DNS     = np.delete(k_DNS, 0)\n",
    "eps_DNS   = np.delete(eps_DNS, 0)\n",
    "dudy_DNS  = np.delete(dudy_DNS, 0)\n",
    "yplus_DNS = np.delete(yplus_DNS,0)\n",
    "uu_DNS    = np.delete(uu_DNS,0)\n",
    "y_DNS     = np.delete(y_DNS,0)\n",
    "u_DNS     = np.delete(u_DNS,0)\n",
    "\n",
    "# Calculate ny_t and time-scale tau\n",
    "viscous_t = k_DNS**2/eps_DNS \n",
    "tau       = viscous_t/abs(uv_DNS)\n",
    "\n",
    "# Calculate c_1, c_2, & c_3 of the Non-linear Eddy Viscosity Model\n",
    "# Array for storing c_1, c_2, & c_3\n",
    "c_0 = -2*(ww_DNS/k_DNS - 2/3)/(tau**2*dudy_DNS**2)\n",
    "c_2 = 2*((ww_DNS/k_DNS - 2/3) + (uu_DNS/k_DNS - 2/3))/(tau**2*dudy_DNS**2)\n",
    "\n",
    "c = np.array([c_0,c_2])\n",
    "\n",
    "#TODO ML using PyTorch to estimate c_1, c_2, & c_3\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up input and output as tensors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stack several arrays horizontally to create a feature matrix\n",
    "from sklearn.discriminant_analysis import StandardScaler\n",
    "\n",
    "scaler1 = StandardScaler()\n",
    "scaler2 = StandardScaler()\n",
    "\n",
    "X = np.column_stack((dudy_DNS**2,uu_DNS))\n",
    "\n",
    "# transpose the target vector to make it a column vector\n",
    "y = c.transpose()\n",
    "\n",
    "# split the feature matrix and target vector into training and validation sets\n",
    "# test_size=0.2 means we reserve 20% of the data for validation\n",
    "# random_state=42 is a fixed seed for the random number generator, ensuring reproducibility\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train_scaled = scaler1.fit_transform(X_train)\n",
    "X_val_scaled   = scaler2.fit_transform(X_val)\n",
    "\n",
    "\n",
    "# convert the numpy arrays to PyTorch tensors with float32 data type\n",
    "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val_scaled, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.float32)\n",
    "\n",
    "# create PyTorch datasets and dataloaders for the training and validation sets\n",
    "# a TensorDataset wraps the feature and target tensors into a single dataset\n",
    "# a DataLoader loads the data in batches and shuffles the batches if shuffle=True\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set up a neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        \n",
    "        super(TestNetwork, self).__init__()\n",
    "\n",
    "        self.input   = nn.Linear(2, 50)     # 1 inputs, 50 outputs\n",
    "        self.hidden1 = nn.Linear(50, 25)    # 50 inputs, 25 outputs\n",
    "        self.hidden2 = nn.Linear(25, 2)     # 25 inputs, 2 outputs\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.functional.relu(self.input(x))\n",
    "        x = nn.functional.relu(self.hidden1(x))\n",
    "        x = self.hidden2(x)\n",
    "        return x\n",
    "    \n",
    "    # Create an instance of the network\n",
    "net = TestNetwork()\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "# Train the network\n",
    "for epoch in range(100):\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        \n",
    "        # Get the inputs and labels\n",
    "        inputs, labels = data\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 10 == 9:\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 10))\n",
    "            running_loss = 0.0\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
